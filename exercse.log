      exit 1
    fi
    if [ "${OS}" != "linux" ]; then
      echo "Signature verification is currently only supported on Linux."
      echo "Please set VERIFY_SIGNATURES=false or verify the signatures manually."
      exit 1
    fi
  fi

  if [ "${HAS_GIT}" != "true" ]; then
    echo "[WARNING] Could not find git. It is required for plugin installation."
  fi
}

# checkDesiredVersion checks if the desired version is available.
checkDesiredVersion() {
  if [ "x$DESIRED_VERSION" == "x" ]; then
    # Get tag from release URL
    local latest_release_url="https://get.helm.sh/helm-latest-version"
    local latest_release_response=""
    if [ "${HAS_CURL}" == "true" ]; then
      latest_release_response=$( curl -L --silent --show-error --fail "$latest_release_url" 2>&1 || true )
    elif [ "${HAS_WGET}" == "true" ]; then
      latest_release_response=$( wget "$latest_release_url" -q -O - 2>&1 || true )
    fi
    TAG=$( echo "$latest_release_response" | grep '^v[0-9]' )
    if [ "x$TAG" == "x" ]; then
      printf "Could not retrieve the latest release tag information from %s: %s\n" "${latest_release_url}" "${latest_release_response}"
      exit 1
    fi
  else
    TAG=$DESIRED_VERSION
  fi
}

# checkHelmInstalledVersion checks which version of helm is installed and
# if it needs to be changed.
checkHelmInstalledVersion() {
  if [[ -f "${HELM_INSTALL_DIR}/${BINARY_NAME}" ]]; then
    local version=$("${HELM_INSTALL_DIR}/${BINARY_NAME}" version --template="{{ .Version }}")
    if [[ "$version" == "$TAG" ]]; then
      echo "Helm ${version} is already ${DESIRED_VERSION:-latest}"
      return 0
    else
      echo "Helm ${TAG} is available. Changing from version ${version}."
      return 1
    fi
  else
    return 1
  fi
}

# downloadFile downloads the latest binary package and also the checksum
# for that binary.
downloadFile() {
  HELM_DIST="helm-$TAG-$OS-$ARCH.tar.gz"
  DOWNLOAD_URL="https://get.helm.sh/$HELM_DIST"
  CHECKSUM_URL="$DOWNLOAD_URL.sha256"
  HELM_TMP_ROOT="$(mktemp -dt helm-installer-XXXXXX)"
  HELM_TMP_FILE="$HELM_TMP_ROOT/$HELM_DIST"
  HELM_SUM_FILE="$HELM_TMP_ROOT/$HELM_DIST.sha256"
  echo "Downloading $DOWNLOAD_URL"
  if [ "${HAS_CURL}" == "true" ]; then
    curl -SsL "$CHECKSUM_URL" -o "$HELM_SUM_FILE"
    curl -SsL "$DOWNLOAD_URL" -o "$HELM_TMP_FILE"
  elif [ "${HAS_WGET}" == "true" ]; then
    wget -q -O "$HELM_SUM_FILE" "$CHECKSUM_URL"
    wget -q -O "$HELM_TMP_FILE" "$DOWNLOAD_URL"
  fi
}

# verifyFile verifies the SHA256 checksum of the binary package
# and the GPG signatures for both the package and checksum file
# (depending on settings in environment).
verifyFile() {
  if [ "${VERIFY_CHECKSUM}" == "true" ]; then
    verifyChecksum
  fi
  if [ "${VERIFY_SIGNATURES}" == "true" ]; then
    verifySignatures
  fi
}

# installFile installs the Helm binary.
installFile() {
  HELM_TMP="$HELM_TMP_ROOT/$BINARY_NAME"
  mkdir -p "$HELM_TMP"
  tar xf "$HELM_TMP_FILE" -C "$HELM_TMP"
  HELM_TMP_BIN="$HELM_TMP/$OS-$ARCH/helm"
  echo "Preparing to install $BINARY_NAME into ${HELM_INSTALL_DIR}"
  runAsRoot cp "$HELM_TMP_BIN" "$HELM_INSTALL_DIR/$BINARY_NAME"
  echo "$BINARY_NAME installed into $HELM_INSTALL_DIR/$BINARY_NAME"
}

# verifyChecksum verifies the SHA256 checksum of the binary package.
verifyChecksum() {
  printf "Verifying checksum... "
  local sum=$(openssl sha1 -sha256 ${HELM_TMP_FILE} | awk '{print $2}')
  local expected_sum=$(cat ${HELM_SUM_FILE})
  if [ "$sum" != "$expected_sum" ]; then
    echo "SHA sum of ${HELM_TMP_FILE} does not match. Aborting."
    exit 1
  fi
  echo "Done."
}

# verifySignatures obtains the latest KEYS file from GitHub main branch
# as well as the signature .asc files from the specific GitHub release,
# then verifies that the release artifacts were signed by a maintainer's key.
verifySignatures() {
  printf "Verifying signatures... "
  local keys_filename="KEYS"
  local github_keys_url="https://raw.githubusercontent.com/helm/helm/main/${keys_filename}"
  if [ "${HAS_CURL}" == "true" ]; then
    curl -SsL "${github_keys_url}" -o "${HELM_TMP_ROOT}/${keys_filename}"
  elif [ "${HAS_WGET}" == "true" ]; then
    wget -q -O "${HELM_TMP_ROOT}/${keys_filename}" "${github_keys_url}"
  fi
  local gpg_keyring="${HELM_TMP_ROOT}/keyring.gpg"
  local gpg_homedir="${HELM_TMP_ROOT}/gnupg"
  mkdir -p -m 0700 "${gpg_homedir}"
  local gpg_stderr_device="/dev/null"
  if [ "${DEBUG}" == "true" ]; then
    gpg_stderr_device="/dev/stderr"
  fi
  gpg --batch --quiet --homedir="${gpg_homedir}" --import "${HELM_TMP_ROOT}/${keys_filename}" 2> "${gpg_stderr_device}"
  gpg --batch --no-default-keyring --keyring "${gpg_homedir}/${GPG_PUBRING}" --export > "${gpg_keyring}"
  local github_release_url="https://github.com/helm/helm/releases/download/${TAG}"
  if [ "${HAS_CURL}" == "true" ]; then
    curl -SsL "${github_release_url}/helm-${TAG}-${OS}-${ARCH}.tar.gz.sha256.asc" -o "${HELM_TMP_ROOT}/helm-${TAG}-${OS}-${ARCH}.tar.gz.sha256.asc"
    curl -SsL "${github_release_url}/helm-${TAG}-${OS}-${ARCH}.tar.gz.asc" -o "${HELM_TMP_ROOT}/helm-${TAG}-${OS}-${ARCH}.tar.gz.asc"
  elif [ "${HAS_WGET}" == "true" ]; then
    wget -q -O "${HELM_TMP_ROOT}/helm-${TAG}-${OS}-${ARCH}.tar.gz.sha256.asc" "${github_release_url}/helm-${TAG}-${OS}-${ARCH}.tar.gz.sha256.asc"
    wget -q -O "${HELM_TMP_ROOT}/helm-${TAG}-${OS}-${ARCH}.tar.gz.asc" "${github_release_url}/helm-${TAG}-${OS}-${ARCH}.tar.gz.asc"
  fi
  local error_text="If you think this might be a potential security issue,"
  error_text="${error_text}\nplease see here: https://github.com/helm/community/blob/master/SECURITY.md"
  local num_goodlines_sha=$(gpg --verify --keyring="${gpg_keyring}" --status-fd=1 "${HELM_TMP_ROOT}/helm-${TAG}-${OS}-${ARCH}.tar.gz.sha256.asc" 2> "${gpg_stderr_device}" | grep -c -E '^\[GNUPG:\] (GOODSIG|VALIDSIG)')
  if [[ ${num_goodlines_sha} -lt 2 ]]; then
    echo "Unable to verify the signature of helm-${TAG}-${OS}-${ARCH}.tar.gz.sha256!"
    echo -e "${error_text}"
    exit 1
  fi
  local num_goodlines_tar=$(gpg --verify --keyring="${gpg_keyring}" --status-fd=1 "${HELM_TMP_ROOT}/helm-${TAG}-${OS}-${ARCH}.tar.gz.asc" 2> "${gpg_stderr_device}" | grep -c -E '^\[GNUPG:\] (GOODSIG|VALIDSIG)')
  if [[ ${num_goodlines_tar} -lt 2 ]]; then
    echo "Unable to verify the signature of helm-${TAG}-${OS}-${ARCH}.tar.gz!"
    echo -e "${error_text}"
    exit 1
  fi
  echo "Done."
}

# fail_trap is executed if an error occurs.
fail_trap() {
  result=$?
  if [ "$result" != "0" ]; then
    if [[ -n "$INPUT_ARGUMENTS" ]]; then
      echo "Failed to install $BINARY_NAME with the arguments provided: $INPUT_ARGUMENTS"
      help
    else
      echo "Failed to install $BINARY_NAME"
    fi
    echo -e "\tFor support, go to https://github.com/helm/helm."
  fi
  cleanup
  exit $result
}

# testVersion tests the installed client to make sure it is working.
testVersion() {
  set +e
  HELM="$(command -v $BINARY_NAME)"
  if [ "$?" = "1" ]; then
    echo "$BINARY_NAME not found. Is $HELM_INSTALL_DIR on your "'$PATH?'
    exit 1
  fi
  set -e
}

# help provides possible cli installation arguments
help () {
  echo "Accepted cli arguments are:"
  echo -e "\t[--help|-h ] ->> prints this help"
  echo -e "\t[--version|-v <desired_version>] . When not defined it fetches the latest release from GitHub"
  echo -e "\te.g. --version v3.0.0 or -v canary"
  echo -e "\t[--no-sudo]  ->> install without sudo"
}

# cleanup temporary files to avoid https://github.com/helm/helm/issues/2977
cleanup() {
  if [[ -d "${HELM_TMP_ROOT:-}" ]]; then
    rm -rf "$HELM_TMP_ROOT"
  fi
}

# Execution

#Stop execution on any error
trap "fail_trap" EXIT
set -e

# Set debug if desired
if [ "${DEBUG}" == "true" ]; then
  set -x
fi

# Parsing input arguments (if any)
export INPUT_ARGUMENTS="${@}"
set -u
while [[ $# -gt 0 ]]; do
  case $1 in
    '--version'|-v)
       shift
       if [[ $# -ne 0 ]]; then
           export DESIRED_VERSION="${1}"
           if [[ "$1" != "v"* ]]; then
               echo "Expected version arg ('${DESIRED_VERSION}') to begin with 'v', fixing..."
               export DESIRED_VERSION="v${1}"
           fi
       else
           echo -e "Please provide the desired version. e.g. --version v3.0.0 or -v canary"
           exit 0
       fi
       ;;
    '--no-sudo')
       USE_SUDO="false"
       ;;
    '--help'|-h)
       help
       exit 0
       ;;
    *) exit 1
       ;;
  esac
  shift
done
set +u

initArch
initOS
verifySupported
checkDesiredVersion
if ! checkHelmInstalledVersion; then
  downloadFile
  verifyFile
  installFile
fi
testVersion
cleanup
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ chmod 700 get_helm.sh
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ ./get_helm.sh
Downloading https://get.helm.sh/helm-v3.13.1-linux-amd64.tar.gz
Verifying checksum... Done.
Preparing to install helm into /usr/local/bin
helm installed into /usr/local/bin/helm
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ helm
The Kubernetes package manager

Common actions for Helm:

- helm search:    search for charts
- helm pull:      download a chart to your local directory to view
- helm install:   upload the chart to Kubernetes
- helm list:      list releases of charts

Environment variables:

| Name                               | Description                                                                                       |
|------------------------------------|---------------------------------------------------------------------------------------------------|
| $HELM_CACHE_HOME                   | set an alternative location for storing cached files.                                             |
| $HELM_CONFIG_HOME                  | set an alternative location for storing Helm configuration.                                       |
| $HELM_DATA_HOME                    | set an alternative location for storing Helm data.                                                |
| $HELM_DEBUG                        | indicate whether or not Helm is running in Debug mode                                             |
| $HELM_DRIVER                       | set the backend storage driver. Values are: configmap, secret, memory, sql.                       |
| $HELM_DRIVER_SQL_CONNECTION_STRING | set the connection string the SQL storage driver should use.                                      |
| $HELM_MAX_HISTORY                  | set the maximum number of helm release history.                                                   |
| $HELM_NAMESPACE                    | set the namespace used for the helm operations.                                                   |
| $HELM_NO_PLUGINS                   | disable plugins. Set HELM_NO_PLUGINS=1 to disable plugins.                                        |
| $HELM_PLUGINS                      | set the path to the plugins directory                                                             |
| $HELM_REGISTRY_CONFIG              | set the path to the registry config file.                                                         |
| $HELM_REPOSITORY_CACHE             | set the path to the repository cache directory                                                    |
| $HELM_REPOSITORY_CONFIG            | set the path to the repositories file.                                                            |
| $KUBECONFIG                        | set an alternative Kubernetes configuration file (default "~/.kube/config")                       |
| $HELM_KUBEAPISERVER                | set the Kubernetes API Server Endpoint for authentication                                         |
| $HELM_KUBECAFILE                   | set the Kubernetes certificate authority file.                                                    |
| $HELM_KUBEASGROUPS                 | set the Groups to use for impersonation using a comma-separated list.                             |
| $HELM_KUBEASUSER                   | set the Username to impersonate for the operation.                                                |
| $HELM_KUBECONTEXT                  | set the name of the kubeconfig context.                                                           |
| $HELM_KUBETOKEN                    | set the Bearer KubeToken used for authentication.                                                 |
| $HELM_KUBEINSECURE_SKIP_TLS_VERIFY | indicate if the Kubernetes API server's certificate validation should be skipped (insecure)       |
| $HELM_KUBETLS_SERVER_NAME          | set the server name used to validate the Kubernetes API server certificate                        |
| $HELM_BURST_LIMIT                  | set the default burst limit in the case the server contains many CRDs (default 100, -1 to disable)|

Helm stores cache, configuration, and data based on the following configuration order:

- If a HELM_*_HOME environment variable is set, it will be used
- Otherwise, on systems supporting the XDG base directory specification, the XDG variables will be used
- When no other location is set a default location will be used based on the operating system

By default, the default directories depend on the Operating System. The defaults are listed below:

| Operating System | Cache Path                | Configuration Path             | Data Path               |
|------------------|---------------------------|--------------------------------|-------------------------|
| Linux            | $HOME/.cache/helm         | $HOME/.config/helm             | $HOME/.local/share/helm |
| macOS            | $HOME/Library/Caches/helm | $HOME/Library/Preferences/helm | $HOME/Library/helm      |
| Windows          | %TEMP%\helm               | %APPDATA%\helm                 | %APPDATA%\helm          |

Usage:
  helm [command]

Available Commands:
  completion  generate autocompletion scripts for the specified shell
  create      create a new chart with the given name
  dependency  manage a chart's dependencies
  env         helm client environment information
  get         download extended information of a named release
  help        Help about any command
  history     fetch release history
  install     install a chart
  lint        examine a chart for possible issues
  list        list releases
  package     package a chart directory into a chart archive
  plugin      install, list, or uninstall Helm plugins
  pull        download a chart from a repository and (optionally) unpack it in local directory
  push        push a chart to remote
  registry    login to or logout from a registry
  repo        add, list, remove, update, and index chart repositories
  rollback    roll back a release to a previous revision
  search      search for a keyword in charts
  show        show information of a chart
  status      display the status of the named release
  template    locally render templates
  test        run tests for a release
  uninstall   uninstall a release
  upgrade     upgrade a release
  verify      verify that a chart at the given path has been signed and is valid
  version     print the client version information

Flags:
      --burst-limit int                 client-side default throttling limit (default 100)
      --debug                           enable verbose output
  -h, --help                            help for helm
      --kube-apiserver string           the address and the port for the Kubernetes API server
      --kube-as-group stringArray       group to impersonate for the operation, this flag can be repeated to specify multiple groups.
      --kube-as-user string             username to impersonate for the operation
      --kube-ca-file string             the certificate authority file for the Kubernetes API server connection
      --kube-context string             name of the kubeconfig context to use
      --kube-insecure-skip-tls-verify   if true, the Kubernetes API server's certificate will not be checked for validity. This will make your HTTPS connections insecure
      --kube-tls-server-name string     server name to use for Kubernetes API server certificate validation. If it is not provided, the hostname used to contact the server is used
      --kube-token string               bearer token used for authentication
      --kubeconfig string               path to the kubeconfig file
  -n, --namespace string                namespace scope for this request
      --registry-config string          path to the registry config file (default "/home/labsuser/.config/helm/registry/config.json")
      --repository-cache string         path to the file containing cached repository indexes (default "/home/labsuser/.cache/helm/repository")
      --repository-config string        path to the file containing repository names and URLs (default "/home/labsuser/.config/helm/repositories.yaml")

Use "helm [command] --help" for more information about a command.
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ helm --version
Error: unknown flag: --version
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ helm -version
Error: invalid argument "ersion" for "-v, --v" flag: strconv.ParseInt: parsing "ersion": invalid syntax
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ helm version
version.BuildInfo{Version:"v3.13.1", GitCommit:"3547a4b5bf5edb5478ce352e18858d8a552a4110", GitTreeState:"clean", GoVersion:"go1.20.8"}
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ helm repo add bitnami https://charts.bitnami.com/bitnami
"bitnami" has been added to your repositories
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ helm install my-redis bitnami/redis --version 17.11.2
NAME: my-redis
LAST DEPLOYED: Fri Dec 15 12:48:28 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: redis
CHART VERSION: 17.11.2
APP VERSION: 7.0.11

** Please be patient while the chart is being deployed **

Redis&reg; can be accessed on the following DNS names from within your cluster:

    my-redis-master.default.svc.cluster.local for read/write operations (port 6379)
    my-redis-replicas.default.svc.cluster.local for read-only operations (port 6379)



To get your password run:

    export REDIS_PASSWORD=$(kubectl get secret --namespace default my-redis -o jsonpath="{.data.redis-password}" | base64 -d)

To connect to your Redis&reg; server:

1. Run a Redis&reg; pod that you can use as a client:

   kubectl run --namespace default redis-client --restart='Never'  --env REDIS_PASSWORD=$REDIS_PASSWORD  --image docker.io/bitnami/redis:7.0.11-debian-11-r7 --command -- sleep infinity

   Use the following command to attach to the pod:

   kubectl exec --tty -i redis-client \
   --namespace default -- bash

2. Connect using the Redis&reg; CLI:
   REDISCLI_AUTH="$REDIS_PASSWORD" redis-cli -h my-redis-master
   REDISCLI_AUTH="$REDIS_PASSWORD" redis-cli -h my-redis-replicas

To connect to your database from outside the cluster execute the following commands:

    kubectl port-forward --namespace default svc/my-redis-master 6379:6379 &
    REDISCLI_AUTH="$REDIS_PASSWORD" redis-cli -h 127.0.0.1 -p 6379
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ xport REDIS_PASSWORD=$(kubectl get secret --namespace default my-redis -o jsonpath="{.data.redis-password}" | base64 -d)

Command 'xport' not found, did you mean:

  command 'port' from snap port (1.10.0)

See 'snap info <snapname>' for additional versions.

labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ export REDIS_PASSWORD=$(kubectl get secret --namespace default my-redis -o jsonpath="{.data.redis-password}" | base64 -d)
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl run --namespace default redis-client --restart='Never'  --env REDIS_PASSWORD=$REDIS_PASSWORD  --image docker.io/bitnami/redis:7.0.11-debian-11-r7 --command -- sleep infinity
pod/redis-client created
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl exec --tty -i redis-client --namespace default -- bash
I have no name!@redis-client:/$ REDISCLI_AUTH="$REDIS_PASSWORD" redis-cli -h my-redis-master
my-redis-master:6379> SET my-key my-dummy-value EX 300
OK
my-redis-master:6379> KEYS my-key
1) "my-key"
my-redis-master:6379> GET my-key
"my-dummy-value"
my-redis-master:6379> FLUSHALL
(error) ERR unknown command 'FLUSHALL', with args beginning with: 
my-redis-master:6379> exit
I have no name!@redis-client:/$ k get pods
bash: k: command not found
I have no name!@redis-client:/$ kubectl get pods
bash: kubectl: command not found
I have no name!@redis-client:/$ kubectl         
bash: kubectl: command not found
I have no name!@redis-client:/$ ps -ef 
UID          PID    PPID  C STIME TTY          TIME CMD
1001           1       0  0 12:50 ?        00:00:00 sleep infinity
1001           7       0  0 12:50 pts/0    00:00:00 bash
1001          17       7  0 12:52 pts/0    00:00:00 ps -ef
I have no name!@redis-client:/$ kubectl get pods
bash: kubectl: command not found
I have no name!@redis-client:/$ kubectl         
bash: kubectl: command not found
I have no name!@redis-client:/$ exit
exit
command terminated with exit code 127
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl get pods
NAME                  READY   STATUS    RESTARTS   AGE
my-redis-master-0     1/1     Running   0          4m59s
my-redis-replicas-0   1/1     Running   0          4m59s
my-redis-replicas-1   1/1     Running   0          4m22s
my-redis-replicas-2   1/1     Running   0          3m52s
redis-client          1/1     Running   0          2m52s
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
"prometheus-community" has been added to your repositories
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ helm install my-kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 45.29.0
NAME: my-kube-prometheus-stack
LAST DEPLOYED: Fri Dec 15 12:58:15 2023
NAMESPACE: default
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace default get pods -l "release=my-kube-prometheus-stack"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl --namespace default get pods -l "release=my-kube-prometheus-stack"
NAME                                                          READY   STATUS    RESTARTS   AGE
my-kube-prometheus-stack-prometheus-node-exporter-pc2kf       1/1     Running   0          72s
my-kube-prometheus-stack-operator-6486f8b75f-7d6vs            1/1     Running   0          71s
my-kube-prometheus-stack-kube-state-metrics-c5dfc8568-4f6ls   1/1     Running   0          71s
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl get services -n default | grep kube-prometheus
my-kube-prometheus-stack-prometheus                 ClusterIP   10.43.243.103   <none>        9090/TCP                     94s
my-kube-prometheus-stack-grafana                    ClusterIP   10.43.187.68    <none>        80/TCP                       94s
my-kube-prometheus-stack-kube-state-metrics         ClusterIP   10.43.243.245   <none>        8080/TCP                     94s
my-kube-prometheus-stack-prometheus-node-exporter   ClusterIP   10.43.172.19    <none>        9100/TCP                     94s
my-kube-prometheus-stack-operator                   ClusterIP   10.43.47.197    <none>        443/TCP                      94s
my-kube-prometheus-stack-alertmanager               ClusterIP   10.43.16.162    <none>        9093/TCP                     94s
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl port-forward service/my-kube-prometheus-stack-prometheus 9090:9090 &
[1] 38578
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ Forwarding from 127.0.0.1:9090 -> 9090
Forwarding from [::1]:9090 -> 9090

labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl port-forward service/my-kube-prometheus-stack-grafana 8080:80 &
[2] 40594
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ Forwarding from 127.0.0.1:8080 -> 3000
Forwarding from [::1]:8080 -> 3000
^C
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl port-forward service/my-kube-prometheus-stack-alertmanager 9093:9093 &
[3] 42047
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ Forwarding from 127.0.0.1:9093 -> 9093
Forwarding from [::1]:9093 -> 9093
^C
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ curl -i http://localhost:9090
Handling connection for 9090
HTTP/1.1 302 Found
Content-Type: text/html; charset=utf-8
Location: /graph
Date: Fri, 15 Dec 2023 13:01:58 GMT
Content-Length: 29

<a href="/graph">Found</a>.

labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ curl -i http://localhost:3000
curl: (7) Failed to connect to localhost port 3000: Connection refused
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ curl -i http://localhost:8080
Handling connection for 8080
HTTP/1.1 302 Found
Cache-Control: no-store
Content-Type: text/html; charset=utf-8
Location: /login
X-Content-Type-Options: nosniff
X-Frame-Options: deny
X-Xss-Protection: 1; mode=block
Date: Fri, 15 Dec 2023 13:02:38 GMT
Content-Length: 29

<a href="/login">Found</a>.

labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ curl -i http://localhost:9093
Handling connection for 9093
HTTP/1.1 200 OK
Accept-Ranges: bytes
Cache-Control: no-cache, no-store, must-revalidate
Content-Length: 1654
Content-Type: text/html; charset=utf-8
Expires: 0
Last-Modified: Thu, 01 Jan 1970 00:00:01 GMT
Pragma: no-cache
Date: Fri, 15 Dec 2023 13:02:52 GMT

<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="icon" type="image/x-icon" href="favicon.ico" />
        <title>Alertmanager</title>
    </head>
    <body>
        <script>
            // If there is no trailing slash at the end of the path in the url,
            // add one. This ensures assets like script.js are loaded properly
            if (location.pathname.substr(-1) != '/') {
                location.pathname = location.pathname + '/';
                console.log('added slash');
            }
        </script>
        <script src="script.js"></script>
        <script>
            var app = Elm.Main.init({
                flags: {
                    production: true,
                    firstDayOfWeek: JSON.parse(localStorage.getItem('firstDayOfWeek')),
                    defaultCreator: localStorage.getItem('defaultCreator'),
                    groupExpandAll: JSON.parse(localStorage.getItem('groupExpandAll'))
                }
            });
            app.ports.persistDefaultCreator.subscribe(function(name) {
                localStorage.setItem('defaultCreator', name);
            });
            app.ports.persistGroupExpandAll.subscribe(function(expanded) {
                localStorage.setItem('groupExpandAll', JSON.stringify(expanded));
            });
            app.ports.persistFirstDayOfWeek.subscribe(function(firstDayOfWeek) {
                localStorage.setItem('firstDayOfWeek', JSON.stringify(firstDayOfWeek));
            });
        </script>
    </body>
</html>
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl create namespace data-store
namespace/data-store created
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ helm install solution-one-redis bitnami/redis --version 17.11.2 --namespace data-store --set auth.password=TheRedis123 --set master.disableCommands=
NAME: solution-one-redis
LAST DEPLOYED: Fri Dec 15 13:05:13 2023
NAMESPACE: data-store
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
CHART NAME: redis
CHART VERSION: 17.11.2
APP VERSION: 7.0.11

** Please be patient while the chart is being deployed **

Redis&reg; can be accessed on the following DNS names from within your cluster:

    solution-one-redis-master.data-store.svc.cluster.local for read/write operations (port 6379)
    solution-one-redis-replicas.data-store.svc.cluster.local for read-only operations (port 6379)



To get your password run:

    export REDIS_PASSWORD=$(kubectl get secret --namespace data-store solution-one-redis -o jsonpath="{.data.redis-password}" | base64 -d)

To connect to your Redis&reg; server:

1. Run a Redis&reg; pod that you can use as a client:

   kubectl run --namespace data-store redis-client --restart='Never'  --env REDIS_PASSWORD=$REDIS_PASSWORD  --image docker.io/bitnami/redis:7.0.11-debian-11-r7 --command -- sleep infinity

   Use the following command to attach to the pod:

   kubectl exec --tty -i redis-client \
   --namespace data-store -- bash

2. Connect using the Redis&reg; CLI:
   REDISCLI_AUTH="$REDIS_PASSWORD" redis-cli -h solution-one-redis-master
   REDISCLI_AUTH="$REDIS_PASSWORD" redis-cli -h solution-one-redis-replicas

To connect to your database from outside the cluster execute the following commands:

    kubectl port-forward --namespace data-store svc/solution-one-redis-master 6379:6379 &
    REDISCLI_AUTH="$REDIS_PASSWORD" redis-cli -h 127.0.0.1 -p 6379
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ helm list -n data-store
NAME                    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART           APP VERSION
solution-one-redis      data-store      1               2023-12-15 13:05:13.36513651 +0000 UTC  deployed        redis-17.11.2   7.0.11     
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl get secret --namespace data-store solution-one-redis -o jsonpath="{.data.redis-password}" | base64 -d
TheRedis123labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ export REDIS_PASSWORD=$(kubectl get secret --namespace data-store solution-one-redis -o jsonpath="{.data.redis-password}" | base64 -d)
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl run --namespace data-store redis-client --restart='Never'  --env REDIS_PASSWORD=$REDIS_PASSWORD  --image docker.io/bitnami/redis:7.0.11-debian-11-r7 --command -- sleep infinity
pod/redis-client created
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl exec --tty -i redis-client --namespace data-store -- bash
I have no name!@redis-client:/$ REDISCLI_AUTH="$REDIS_PASSWORD" redis-cli -h solution-one-redis-master.data-store
solution-one-redis-master.data-store:6379> SET my-key my-other-dummy-value EX 300
OK
solution-one-redis-master.data-store:6379> KEYS my-key
1) "my-key"
solution-one-redis-master.data-store:6379> FLUSHALL
OK
solution-one-redis-master.data-store:6379> KEYS my-other-key
(empty array)
solution-one-redis-master.data-store:6379> exit
I have no name!@redis-client:/$ nano values-monitoring.yml
bash: nano: command not found
I have no name!@redis-client:/$ exit
exit
command terminated with exit code 127
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ nano values-monitoring.yml
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ helm install solution-one-kube-prometheus --repo https://prometheus-community.github.io/helm-charts kube-prometheus-stack --namespace monitoring --create-namespace --values values-monitoring.yml
W1215 13:12:55.251264   90724 warnings.go:70] unknown field "spec.scrapeConfigNamespaceSelector"
W1215 13:12:55.251283   90724 warnings.go:70] unknown field "spec.scrapeConfigSelector"
NAME: solution-one-kube-prometheus
LAST DEPLOYED: Fri Dec 15 13:12:39 2023
NAMESPACE: monitoring
STATUS: deployed
REVISION: 1
NOTES:
kube-prometheus-stack has been installed. Check its status by running:
  kubectl --namespace monitoring get pods -l "release=solution-one-kube-prometheus"

Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl get service -n monitoring
NAME                                                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
solution-one-kube-promethe-prometheus                   ClusterIP   10.43.78.122    <none>        9090/TCP,8080/TCP            42s
solution-one-kube-prometheus-grafana                    ClusterIP   10.43.56.175    <none>        80/TCP                       42s
solution-one-kube-promethe-operator                     ClusterIP   10.43.115.135   <none>        443/TCP                      42s
solution-one-kube-prometheus-kube-state-metrics         ClusterIP   10.43.208.188   <none>        8080/TCP                     42s
solution-one-kube-prometheus-prometheus-node-exporter   ClusterIP   10.43.195.206   <none>        9100/TCP                     42s
solution-one-kube-promethe-alertmanager                 ClusterIP   10.43.12.250    <none>        9093/TCP,8080/TCP            42s
alertmanager-operated                                   ClusterIP   None            <none>        9093/TCP,9094/TCP,9094/UDP   40s
prometheus-operated                                     ClusterIP   None            <none>        9090/TCP                     40s
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl get ingress -n monitoring
NAME                                      CLASS    HOSTS   ADDRESS    PORTS   AGE
solution-one-kube-prometheus-grafana      <none>   *       10.0.2.2   80      54s
solution-one-kube-promethe-prometheus     <none>   *       10.0.2.2   80      54s
solution-one-kube-promethe-alertmanager   <none>   *       10.0.2.2   80      54s
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ curl -L -s -o /dev/null -w "%{http_code}\n" http://127.0.0.1/prometheus/metrics
404
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ curl -L -s -o /dev/null -w "%{http_code}\n" http://127.0.0.1/grafana
200
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ curl -L -s -o /dev/null -w "%{http_code}\n" http://127.0.0.1/alertmanager
404
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl get pod -n monitoring | grep kube-prometheus
solution-one-kube-prometheus-prometheus-node-exporter-skkkc       0/1     Pending       0          2m32s
solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x   1/1     Running       0          2m32s
solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g             3/3     Running       0          2m32s
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl get pod -n monitoring | grep kube-prometheus
solution-one-kube-prometheus-prometheus-node-exporter-skkkc       0/1     Pending       0          2m42s
solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x   1/1     Running       0          2m42s
solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g             3/3     Running       0          2m42s
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl get pod -n monitoring | grep kube-prometheus
solution-one-kube-prometheus-prometheus-node-exporter-skkkc       0/1     Pending       0          2m46s
solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x   1/1     Running       0          2m46s
solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g             3/3     Running       0          2m46s
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl get pod -n monitoring | grep kube-prometheus
solution-one-kube-prometheus-prometheus-node-exporter-skkkc       0/1     Pending       0          3m2s
solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x   1/1     Running       0          3m2s
solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g             3/3     Running       0          3m2s
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl get pod -n monitoring 
NAME                                                              READY   STATUS        RESTARTS   AGE
solution-one-kube-prometheus-prometheus-node-exporter-skkkc       0/1     Pending       0          3m18s
solution-one-kube-promethe-operator-84687c7bd9-wfttz              1/1     Running       0          3m18s
solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x   1/1     Running       0          3m18s
solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g             3/3     Running       0          3m18s
prometheus-solution-one-kube-promethe-prometheus-0                0/2     Terminating   0          3s
alertmanager-solution-one-kube-promethe-alertmanager-0            0/2     Pending       0          1s
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ kubectl get events -n monitoring 
LAST SEEN   TYPE      REASON              OBJECT                                                                 MESSAGE
3m44s       Normal    SuccessfulCreate    job/solution-one-kube-promethe-admission-create                        Created pod: solution-one-kube-promethe-admission-create-9dl59
3m44s       Normal    Scheduled           pod/solution-one-kube-promethe-admission-create-9dl59                  Successfully assigned monitoring/solution-one-kube-promethe-admission-create-9dl59 to labs
3m44s       Normal    Pulled              pod/solution-one-kube-promethe-admission-create-9dl59                  Container image "registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6" already present on machine
3m44s       Normal    Created             pod/solution-one-kube-promethe-admission-create-9dl59                  Created container create
3m43s       Normal    Started             pod/solution-one-kube-promethe-admission-create-9dl59                  Started container create
3m40s       Normal    Completed           job/solution-one-kube-promethe-admission-create                        Job completed
3m38s       Warning   FailedScheduling    pod/solution-one-kube-prometheus-prometheus-node-exporter-skkkc        0/1 nodes are available: 1 node(s) didn't have free ports for the requested pod ports. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.
3m39s       Normal    SuccessfulCreate    daemonset/solution-one-kube-prometheus-prometheus-node-exporter        Created pod: solution-one-kube-prometheus-prometheus-node-exporter-skkkc
3m39s       Normal    ScalingReplicaSet   deployment/solution-one-kube-prometheus-kube-state-metrics             Scaled up replica set solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c to 1
3m39s       Normal    ScalingReplicaSet   deployment/solution-one-kube-promethe-operator                         Scaled up replica set solution-one-kube-promethe-operator-84687c7bd9 to 1
3m39s       Normal    ScalingReplicaSet   deployment/solution-one-kube-prometheus-grafana                        Scaled up replica set solution-one-kube-prometheus-grafana-5b99fbf8f8 to 1
3m39s       Normal    SuccessfulCreate    replicaset/solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c   Created pod: solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x
3m38s       Normal    Scheduled           pod/solution-one-kube-promethe-operator-84687c7bd9-wfttz               Successfully assigned monitoring/solution-one-kube-promethe-operator-84687c7bd9-wfttz to labs
3m38s       Normal    Scheduled           pod/solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x    Successfully assigned monitoring/solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x to labs
3m39s       Normal    SuccessfulCreate    replicaset/solution-one-kube-promethe-operator-84687c7bd9              Created pod: solution-one-kube-promethe-operator-84687c7bd9-wfttz
3m38s       Normal    Scheduled           pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Successfully assigned monitoring/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g to labs
3m39s       Normal    SuccessfulCreate    replicaset/solution-one-kube-prometheus-grafana-5b99fbf8f8             Created pod: solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g
3m37s       Warning   FailedMount         pod/solution-one-kube-promethe-operator-84687c7bd9-wfttz               MountVolume.SetUp failed for volume "tls-secret" : failed to sync secret cache: timed out waiting for the condition
3m37s       Warning   FailedMount         pod/solution-one-kube-promethe-operator-84687c7bd9-wfttz               MountVolume.SetUp failed for volume "kube-api-access-9mkbj" : failed to sync configmap cache: timed out waiting for the condition
3m37s       Warning   FailedMount         pod/solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x    MountVolume.SetUp failed for volume "kube-api-access-hvrf6" : failed to sync configmap cache: timed out waiting for the condition
3m37s       Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
3m36s       Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
3m36s       Normal    Pulling             pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Pulling image "quay.io/kiwigrid/k8s-sidecar:1.25.2"
3m35s       Normal    Pulling             pod/solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x    Pulling image "registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1"
3m35s       Normal    Pulling             pod/solution-one-kube-promethe-operator-84687c7bd9-wfttz               Pulling image "quay.io/prometheus-operator/prometheus-operator:v0.70.0"
3m34s       Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
3m33s       Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
3m33s       Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
3m33s       Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
3m33s       Normal    SuccessfulCreate    job/solution-one-kube-promethe-admission-patch                         Created pod: solution-one-kube-promethe-admission-patch-hvhvd
3m33s       Normal    Scheduled           pod/solution-one-kube-promethe-admission-patch-hvhvd                   Successfully assigned monitoring/solution-one-kube-promethe-admission-patch-hvhvd to labs
3m32s       Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
3m32s       Normal    Pulling             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Pulling image "quay.io/prometheus/alertmanager:v0.26.0"
3m32s       Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
3m32s       Normal    Pulled              pod/solution-one-kube-promethe-admission-patch-hvhvd                   Container image "registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20221220-controller-v1.5.1-58-g787ea74b6" already present on machine
3m32s       Normal    Created             pod/solution-one-kube-promethe-admission-patch-hvhvd                   Created container patch
3m31s       Normal    Pulling             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Pulling image "quay.io/prometheus/prometheus:v2.48.1"
3m31s       Normal    Started             pod/solution-one-kube-promethe-admission-patch-hvhvd                   Started container patch
3m30s       Normal    Pulled              pod/solution-one-kube-promethe-operator-84687c7bd9-wfttz               Successfully pulled image "quay.io/prometheus-operator/prometheus-operator:v0.70.0" in 4.742417795s
3m30s       Normal    Created             pod/solution-one-kube-promethe-operator-84687c7bd9-wfttz               Created container kube-prometheus-stack
3m30s       Normal    Started             pod/solution-one-kube-promethe-operator-84687c7bd9-wfttz               Started container kube-prometheus-stack
3m27s       Normal    Pulled              pod/solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x    Successfully pulled image "registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1" in 8.804058756s
3m26s       Normal    Pulled              pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Successfully pulled image "quay.io/kiwigrid/k8s-sidecar:1.25.2" in 9.890377324s
3m26s       Normal    Created             pod/solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x    Created container kube-state-metrics
3m26s       Normal    Created             pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Created container grafana-sc-dashboard
3m26s       Normal    Started             pod/solution-one-kube-prometheus-kube-state-metrics-8f46f8f9c-4th9x    Started container kube-state-metrics
3m26s       Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully pulled image "quay.io/prometheus/alertmanager:v0.26.0" in 6.802563483s
3m25s       Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container alertmanager
3m25s       Normal    Started             pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Started container grafana-sc-dashboard
3m25s       Normal    Pulled              pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Container image "quay.io/kiwigrid/k8s-sidecar:1.25.2" already present on machine
3m25s       Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container alertmanager
3m25s       Normal    Created             pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Created container grafana-sc-datasources
3m25s       Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
3m25s       Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container config-reloader
3m24s       Normal    Started             pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Started container grafana-sc-datasources
3m24s       Normal    Pulling             pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Pulling image "docker.io/grafana/grafana:10.2.2"
3m24s       Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container config-reloader
3m23s       Warning   FailedScheduling    pod/solution-one-kube-prometheus-prometheus-node-exporter-skkkc        0/1 nodes are available: 1 node(s) didn't have free ports for the requested pod ports. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.
3m24s       Normal    Killing             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Stopping container alertmanager
3m24s       Normal    Killing             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Stopping container config-reloader
3m18s       Normal    Completed           job/solution-one-kube-promethe-admission-patch                         Job completed
3m16s       Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
3m15s       Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully pulled image "quay.io/prometheus/prometheus:v2.48.1" in 16.70272361s
3m15s       Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container prometheus
3m14s       Normal    Pulling             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Pulling image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0"
3m14s       Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container prometheus
3m14s       Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
3m14s       Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container config-reloader
3m13s       Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container config-reloader
3m13s       Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container prometheus
3m13s       Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container config-reloader
3m12s       Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully pulled image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" in 2.915551917s
3m11s       Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
3m11s       Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
3m7s        Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
3m7s        Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
3m5s        Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
3m5s        Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
3m5s        Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
3m4s        Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
3m4s        Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
3m4s        Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
3m2s        Normal    Pulled              pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Successfully pulled image "docker.io/grafana/grafana:10.2.2" in 22.017906204s
3m2s        Normal    Created             pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Created container grafana
3m2s        Normal    Started             pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Started container grafana
2m59s       Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
2m57s       Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
2m57s       Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
2m57s       Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
2m57s       Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
2m57s       Warning   Unhealthy           pod/solution-one-kube-prometheus-grafana-5b99fbf8f8-4td6g              Readiness probe failed: Get "http://10.42.0.35:3000/api/health": dial tcp 10.42.0.35:3000: connect: connection refused
2m55s       Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
2m55s       Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
2m54s       Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
2m54s       Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container init-config-reloader
2m52s       Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
2m49s       Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
2m49s       Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
2m49s       Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
2m47s       Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
2m45s       Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
2m45s       Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
2m45s       Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
2m43s       Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
2m42s       Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
2m42s       Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
2m42s       Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
2m41s       Normal    Killing             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Stopping container init-config-reloader
2m38s       Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
2m36s       Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
2m35s       Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
2m35s       Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
2m35s       Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
2m34s       Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
2m34s       Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
2m33s       Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
2m30s       Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
2m29s       Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
2m29s       Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
2m28s       Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
2m28s       Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
2m27s       Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
2m27s       Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
2m26s       Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
2m26s       Normal    Killing             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Stopping container init-config-reloader
2m22s       Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
2m20s       Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
2m20s       Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
2m20s       Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
2m20s       Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
2m18s       Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
2m18s       Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
2m18s       Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
2m15s       Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
2m13s       Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
2m13s       Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
2m13s       Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
2m13s       Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
2m13s       Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container init-config-reloader
2m12s       Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
2m12s       Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
2m12s       Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
2m7s        Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
2m5s        Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
2m5s        Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
2m5s        Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
2m4s        Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
2m2s        Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
2m2s        Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
2m2s        Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
119s        Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
119s        Normal    SuccessfulDelete    statefulset/alertmanager-solution-one-kube-promethe-alertmanager       delete Pod alertmanager-solution-one-kube-promethe-alertmanager-0 in StatefulSet alertmanager-solution-one-kube-promethe-alertmanager successful
118s        Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
118s        Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
118s        Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
114s        Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
112s        Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
112s        Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
112s        Normal    SuccessfulCreate    statefulset/alertmanager-solution-one-kube-promethe-alertmanager       create Pod alertmanager-solution-one-kube-promethe-alertmanager-0 in StatefulSet alertmanager-solution-one-kube-promethe-alertmanager successful
112s        Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
112s        Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
110s        Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
110s        Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
110s        Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
106s        Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
105s        Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
105s        Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
104s        Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
101s        Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
100s        Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
99s         Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
99s         Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
99s         Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
98s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
98s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
98s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
97s         Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container init-config-reloader
94s         Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
93s         Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
93s         Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
92s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
92s         Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
92s         Normal    SuccessfulDelete    statefulset/prometheus-solution-one-kube-promethe-prometheus           delete Pod prometheus-solution-one-kube-promethe-prometheus-0 in StatefulSet prometheus-solution-one-kube-promethe-prometheus successful
89s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
89s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
88s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
87s         Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
88s         Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container init-config-reloader
86s         Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
85s         Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
85s         Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
85s         Normal    SuccessfulCreate    statefulset/prometheus-solution-one-kube-promethe-prometheus           create Pod prometheus-solution-one-kube-promethe-prometheus-0 in StatefulSet prometheus-solution-one-kube-promethe-prometheus successful
84s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
83s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
83s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
83s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
83s         Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container init-config-reloader
81s         Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
80s         Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
80s         Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
80s         Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
76s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
75s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
75s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
74s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
71s         Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
69s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
69s         Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
69s         Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
69s         Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
68s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
67s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
67s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
66s         Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
64s         Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
63s         Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
63s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
63s         Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
62s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
62s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
61s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
60s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus/prometheus:v2.48.1" already present on machine
60s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container prometheus
60s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container prometheus
59s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
59s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container config-reloader
59s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container config-reloader
56s         Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
56s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
56s         Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container config-reloader
56s         Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container prometheus
55s         Warning   FailedMount         pod/prometheus-solution-one-kube-promethe-prometheus-0                 MountVolume.SetUp failed for volume "web-config" : failed to sync secret cache: timed out waiting for the condition
55s         Warning   FailedMount         pod/prometheus-solution-one-kube-promethe-prometheus-0                 MountVolume.SetUp failed for volume "prometheus-solution-one-kube-promethe-prometheus-rulefiles-0" : failed to sync configmap cache: timed out waiting for the condition
55s         Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
55s         Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
55s         Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
54s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
54s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
53s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
49s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
49s         Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
48s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
48s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
48s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
48s         Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container init-config-reloader
47s         Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
47s         Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
46s         Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
43s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
42s         Warning   FailedMount         pod/prometheus-solution-one-kube-promethe-prometheus-0                 MountVolume.SetUp failed for volume "prometheus-solution-one-kube-promethe-prometheus-rulefiles-0" : failed to sync configmap cache: timed out waiting for the condition
41s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
41s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
41s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
40s         Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container init-config-reloader
38s         Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
37s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
37s         Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
37s         Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
37s         Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
35s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
35s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
34s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
32s         Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
30s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
30s         Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
30s         Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
30s         Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
29s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
28s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
28s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
28s         Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container init-config-reloader
24s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
22s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
21s         Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
22s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
21s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
21s         Normal    Killing             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Stopping container init-config-reloader
20s         Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.65.1" already present on machine
20s         Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
20s         Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
19s         Normal    Killing             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Stopping container init-config-reloader
14s         Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
13s         Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
13s         Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
13s         Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
11s         Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
9s          Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
9s          Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
8s          Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
7s          Normal    Scheduled           pod/prometheus-solution-one-kube-promethe-prometheus-0                 Successfully assigned monitoring/prometheus-solution-one-kube-promethe-prometheus-0 to labs
6s          Normal    Pulled              pod/prometheus-solution-one-kube-promethe-prometheus-0                 Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
5s          Normal    Created             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Created container init-config-reloader
5s          Normal    Started             pod/prometheus-solution-one-kube-promethe-prometheus-0                 Started container init-config-reloader
3s          Normal    Scheduled           pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Successfully assigned monitoring/alertmanager-solution-one-kube-promethe-alertmanager-0 to labs
1s          Normal    Pulled              pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Container image "quay.io/prometheus-operator/prometheus-config-reloader:v0.70.0" already present on machine
1s          Normal    Created             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Created container init-config-reloader
1s          Normal    Started             pod/alertmanager-solution-one-kube-promethe-alertmanager-0             Started container init-config-reloader
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ history
    1  To join the meeting on Google Meet, click this link: https://meet.google.com/qjz-pmgx-xav
    2  curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
    3  chmod 700 get_helm.sh
    4  cat get_helm.sh
    5  chmod 700 get_helm.sh
    6  ./get_helm.sh
    7  helm
    8  helm --version
    9  helm -version
   10  helm version
   11  helm repo add bitnami https://charts.bitnami.com/bitnami
   12  helm install my-redis bitnami/redis --version 17.11.2
   13  xport REDIS_PASSWORD=$(kubectl get secret --namespace default my-redis -o jsonpath="{.data.redis-password}" | base64 -d)
   14  export REDIS_PASSWORD=$(kubectl get secret --namespace default my-redis -o jsonpath="{.data.redis-password}" | base64 -d)
   15  kubectl run --namespace default redis-client --restart='Never'  --env REDIS_PASSWORD=$REDIS_PASSWORD  --image docker.io/bitnami/redis:7.0.11-debian-11-r7 --command -- sleep infinity
   16  kubectl exec --tty -i redis-client --namespace default -- bash
   17  kubectl get pods
   18  helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
   19  helm install my-kube-prometheus-stack prometheus-community/kube-prometheus-stack --version 45.29.0
   20  kubectl --namespace default get pods -l "release=my-kube-prometheus-stack"
   21  kubectl get services -n default | grep kube-prometheus
   22  kubectl port-forward service/my-kube-prometheus-stack-prometheus 9090:9090 &
   23  kubectl port-forward service/my-kube-prometheus-stack-grafana 8080:80 &
   24  kubectl port-forward service/my-kube-prometheus-stack-alertmanager 9093:9093 &
   25  curl -i http://localhost:9090
   26  curl -i http://localhost:3000
   27  curl -i http://localhost:8080
   28  curl -i http://localhost:9093
   29  kubectl create namespace data-store
   30  helm install solution-one-redis bitnami/redis --version 17.11.2 --namespace data-store --set auth.password=TheRedis123 --set master.disableCommands=
   31  helm list -n data-store
   32  kubectl get secret --namespace data-store solution-one-redis -o jsonpath="{.data.redis-password}" | base64 -d
   33  export REDIS_PASSWORD=$(kubectl get secret --namespace data-store solution-one-redis -o jsonpath="{.data.redis-password}" | base64 -d)
   34  kubectl run --namespace data-store redis-client --restart='Never'  --env REDIS_PASSWORD=$REDIS_PASSWORD  --image docker.io/bitnami/redis:7.0.11-debian-11-r7 --command -- sleep infinity
   35  kubectl exec --tty -i redis-client --namespace data-store -- bash
   36  nano values-monitoring.yml
   37  helm install solution-one-kube-prometheus --repo https://prometheus-community.github.io/helm-charts kube-prometheus-stack --namespace monitoring --create-namespace --values values-monitoring.yml
   38  kubectl get service -n monitoring
   39  kubectl get ingress -n monitoring
   40  curl -L -s -o /dev/null -w "%{http_code}\n" http://127.0.0.1/prometheus/metrics
   41  curl -L -s -o /dev/null -w "%{http_code}\n" http://127.0.0.1/grafana
   42  curl -L -s -o /dev/null -w "%{http_code}\n" http://127.0.0.1/alertmanager
   43  kubectl get pod -n monitoring | grep kube-prometheus
   44  kubectl get pod -n monitoring 
   45  kubectl get events -n monitoring 
   46  history
labsuser@labs-vm-4549b7f5-ebd2-4d63-b849-22392827f68d:~$ 
